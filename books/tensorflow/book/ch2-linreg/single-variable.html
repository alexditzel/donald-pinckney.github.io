<!DOCTYPE HTML>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>Single Variable Regression - Machine Learning with TensorFlow</title>
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="An introductory look at implementing machine learning algorithms using TensorFlow in Python.">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <base href="../">

        <link rel="stylesheet" href="book.css">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css">
        <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:500" rel="stylesheet" type="text/css">

        <link rel="shortcut icon" href="favicon.png">

        <!-- Font Awesome -->
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme -->
        

        
        <!-- MathJax -->
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        

        <!-- Fetch Clipboard.js from CDN but have a local fallback -->
        <script src="https://cdn.jsdelivr.net/clipboard.js/1.6.1/clipboard.min.js"></script>
        <script>
            if (typeof Clipboard == 'undefined') {
                document.write(unescape("%3Cscript src='clipboard.min.js'%3E%3C/script%3E"));
            }
        </script>

        <!-- Fetch JQuery from CDN but have a local fallback -->
        <script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
        <script>
            if (typeof jQuery == 'undefined') {
                document.write(unescape("%3Cscript src='jquery.js'%3E%3C/script%3E"));
            }
        </script>

        <!-- Fetch store.js from local - TODO add CDN when 2.x.x is available on cdnjs -->
        <script src="store.js"></script>

        <!-- Custom JS script -->
        

    </head>
    <body class="light">
        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme = store.get('mdbook-theme');
            if (theme === null || theme === undefined) { theme = 'light'; }
            $('body').removeClass().addClass(theme);
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var sidebar = store.get('mdbook-sidebar');
            if (sidebar === "hidden") { $("html").addClass("sidebar-hidden") }
            else if (sidebar === "visible") { $("html").addClass("sidebar-visible") }
        </script>

        <div id="sidebar" class="sidebar">
            <ul class="chapter"><li><a href="ch1-setup/intro.html"><strong>1.</strong> Introduction and Setup</a></li><li><ul class="section"><li><a href="ch1-setup/mac.html"><strong>1.1.</strong> macOS Setup</a></li><li><a href="ch1-setup/linux.html"><strong>1.2.</strong> Linux Setup</a></li><li><a href="ch1-setup/windows.html"><strong>1.3.</strong> Windows Setup</a></li></ul></li><li><a href="ch2-linreg/intro.html"><strong>2.</strong> Linear Regression</a></li><li><ul class="section"><li><a href="ch2-linreg/single-variable.html" class="active"><strong>2.1.</strong> Single Variable Regression</a></li><li><strong>2.2.</strong> Multi Variable Regression</li><li class="spacer"></li></ul></li><li><a href="http:/index.html">By Donald Pinckney</a></li></ul>
        </div>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page" tabindex="-1">
                <div id="menu-bar" class="menu-bar">
                    <div class="left-buttons">
                        <i id="sidebar-toggle" class="fa fa-bars"></i>
                        <i id="theme-toggle" class="fa fa-paint-brush"></i>
                    </div>

                    <h1 class="menu-title">Machine Learning with TensorFlow</h1>

                    <div class="right-buttons">
                        <a href="print.html">
                            <i id="print-button" class="fa fa-print" title="Print this book"></i>
                        </a>
                    </div>
                </div>

                <div id="content" class="content">
                    <a class="header" href="ch2-linreg/single-variable.html#single-variable-regression" id="single-variable-regression"><h1>Single Variable Regression</h1></a>
<p>Since this is the very first tutorial in this book and no knowledge is assumed about machine learning or TensorFlow, this tutorial is a bit on the long side. This tutorial will give you an overview of how to do machine learning work in general, a mathematical understanding of single variable linear regression, and how to implement it in TensorFlow. If you already feel comfortable with the mathematical concept of linear regression, feel free to skip to the TensorFlow <a href="ch2-linreg/single-variable.html#implementation">implementation</a>.</p>
<a class="header" href="ch2-linreg/single-variable.html#motivation" id="motivation"><h2>Motivation</h2></a>
<p>Single variable linear regression is one of the fundamental tools for any interpretation of data. Using linear regression, we can predict continuous variable outcomes given some data, if the data has a roughly linear shape, i.e. it generally has the shape a line. For example, consider the plot below of 2015 US homicide deaths per age<sup class="footnote-reference"><a href="ch2-linreg/single-variable.html#fn1">1</a></sup>, and the line of best fit next to it.</p>
<table><thead><tr><th align="center">Original data              </th><th align="center">  Result of single variable linear regression</th></tr></thead><tbody>
<tr><td align="center"><img src="/books/tensorflow/book/ch2-linreg/assets/homicide.png" alt="Homicide Plot" /> </td><td align="center"> <img src="/books/tensorflow/book/ch2-linreg/assets/homicide_fit.png" alt="Homicide Regression Plot" /></td></tr>
</tbody></table>
<p>Visually, it appears that this data is approximated pretty well by a &quot;line of best fit&quot;. Single variable linear regression is precisely the tool to find this line. The line of best fit can then be used to guess how many homicide deaths there would be for ages we don't have data on. By the end of this tutorial you can run linear regression on this homicide data, and in fact solve any single variable regression problem.</p>
<a class="header" href="ch2-linreg/single-variable.html#theory" id="theory"><h2>Theory</h2></a>
<p>Since we don't have any theory yet to understand linear regression, first we need to develop the theory necessary to program it.</p>
<a class="header" href="ch2-linreg/single-variable.html#data-set-format" id="data-set-format"><h3>Data set format</h3></a>
<p>For regression problems, the goal is to predict a continuous variable output, given some input variables (usually also continuous). For single variable regression, we only have one input variable, called \(x\), and our <em>desired</em> output \(y\). Our data set \(D\) then consists of many examples of \(x\) and \(y\), so:
\[
D = \{ (x_1, y_1), (x_2, y_2), \cdots, (x_m, y_m) \}
\]
where \(m\) is the number of examples in the data set. For a concrete example, the homicide data set plotted above looks like:
\[
D = \{ (21, 652), (22, 633), \cdots, (50, 197) \}
\]
We will write code to load data sets from files later.</p>
<a class="header" href="ch2-linreg/single-variable.html#model-concept" id="model-concept"><h3>Model concept</h3></a>
<p>So, how can we mathematically model single linear regression? Since the goal is to find the perfect line, let's start by defining the <strong>model</strong> (the mathematical description of how predictions will be created) as a line:
\[
y'(x, a, b) = ax + b
\]
where \(x\) is an input, \(a, b\) are constants, and \(y'\) is the prediction for the input \(x\). Note that although this is an equation for a line with \(x\) as the variable, the values of \(a\) and \(b\) determine what specific line it is. To find the best line, we just need to find the best values for \(a\) (the slope) and \(b\) (the y-intercept). For example, the line of best fit for the homicide data above has a slope of about \(a \approx -17.69\) and a y-intercept of \(b \approx 1000\). How we find the magic best values for \(a\) and \(b\) we don't know yet, but once we find them, prediction is easy, since we just use the formula above.</p>
<p>How then, how do we find the correct values of \(a\) and \(b\)? First, we need a way to define what the &quot;best line&quot; is exactly. To do so, we define a <strong>loss function</strong> (also called a cost function), which measures how bad a particular choice of \(a\) and \(b\) are. Values of \(a\) and \(b\) that seem poor (a line that does not fit the data set) should result in a large value of the loss function, whereas good values of \(a\) and \(b\) (a line that fits the data set well) should result in small values of the loss function. In other words, the loss function should measure how far the predicted line is from each of the data points, and add this value up for all data points. We can write this as:
\[
L(a, b) = \sum_{i=1}^m (y'(x_i, a, b) - y_i)^2
\]
Recall that there are \(m\) examples in the data set, \(x_i\) is the i'th input, and \(y_i\) is the i'th desired output. So, \((y'(x_i, a, b) - y_i)^2\) measures how far the i'th prediction is from the i'th desired output. For example, if the prediction \(y'\) is 7, and the correct output \(y\) is 10, then we would get \((7 - 10)^2 = 9.\) Squaring it is important so that it is always positive.  Finally, we just add up all of these individual losses.</p>
<blockquote>
<p>Note: The choice to square \(y'(x_i, a, b) - y_i\) is somewhat arbitrary. Though we need to make it positive, we could achieve this in many ways, such as taking the absolute value. In sense, the choice of models and loss functions is the creative aspect of machine learning, and often a certain loss function is chosen simply because it produces satisfying results. Manipulating the loss function to achieve more satisfying results will be done in a later section.</p>
</blockquote>
<a class="header" href="ch2-linreg/single-variable.html#optimizing-the-model" id="optimizing-the-model"><h3>Optimizing the model</h3></a>
<p>At this point, we have fully defined both our model:
\[
y'(x, a, b) = ax + b
\]
and our loss function, into which we can substitute the model:
\[
L(a, b) = \sum_{i=1}^m (y'(x_i, a, b) - y_i)^2 = \sum_{i=1}^m (a x_i + b - y_i)^2
\]
We crafted \(L(a, b)\) so that it is smallest exactly when \(a\) and \(b\) produce the line of best fit. Therefore, our goal is to find the values of \(a\) and \(b\) that minimize the function \(L(a, b)\). But what does \(L\) really look like? Well, it is essentially a 3D parabola which looks like:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/minimum.png" alt="Minimum Plot" /></p>
<p>The red dot marked on the plot of \(L\) shows where the desired minimum is. We need an algorithm to find this minimum. From calculus, we know that at the minimum \(L\) must be entirely flat, that is the derivatives are both \(0\):
\[
\frac{\partial L}{\partial a} = \sum_{i=1}^m 2(ax_i + b - y_i)x_i = 0 \\
\frac{\partial L}{\partial b} = \sum_{i=1}^m 2(ax_i + b - y_i) = 0 \
\]
If you need to review this aspect of calculus, I would recommend <a href="https://www.khanacademy.org/math/differential-calculus/analyzing-func-with-calc-dc">Khan Academy videos</a>. Now, for this problem it is possible to solve for \(a\) and \(b\) using the equations above, like we would in a typical calculus course. But for more advanced machine learning this is impossible, so instead we will learn to use an algorithm called <em><a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a></em> to find the minimum. The idea is intuitive: place a ball at an arbitrary location on the surface of \(L\), and it will naturally roll downhill towards the flat valley of \(L\) and thus find the minimum. We know the direction of &quot;downhill&quot; at any location since we know the derivatives of \(L\): the derivatives are the direction of greatest upward slope, so the opposite (negative) derivatives are the most downhill direction. Therefore, if the ball is currently at location \((a, b)\), we can simulate it's motion by moving it to location \((a', b')\) like so:
\[
a' = a - \alpha \frac{\partial L}{\partial a} \\
b' = b - \alpha \frac{\partial L}{\partial b} \\
\]
where \(\alpha\) is a constant called the <strong>learning rate</strong>, which we will talk about more later. If we repeat this process then the ball will continue to roll downhill into the minimum. An animation of this process looks like:</p>
<video autoplay loop muted>
<source type="video/mp4" src="/books/tensorflow/book/ch2-linreg/assets/descent_fast.mp4">
</video>
<p>When we run the gradient descent algorithm for long enough, then it will find the optimal location for \((a, b)\). Once we have the optimal values of \(a\) and \(b\), then that's it, we can just use them to predict a rate of homicide deaths given any age, using the model:
\[
y'(x) = ax + b
\]</p>
<a class="header" href="ch2-linreg/single-variable.html#implementation" id="implementation"><h2>Implementation</h2></a>
<p>Let's quickly review what we did when defining the theory of linear regression:</p>
<ol>
<li>Describe the data set</li>
<li>Define the model</li>
<li>Define the loss function</li>
<li>Run the gradient descent optimization algorithm</li>
<li>Use the optimal model to make predictions</li>
<li>Profit!</li>
</ol>
<p>When coding this we will follow the exact same steps. So, create a new file <code>single_var_reg.py</code> in the text editor or IDE of your choice (or experiment in the Python REPL), and download the <a href="/books/tensorflow/book/ch2-linreg/code/homicide.csv">homicide death rate data set</a> into the same directory.</p>
<a class="header" href="ch2-linreg/single-variable.html#importing-the-data" id="importing-the-data"><h3>Importing the data</h3></a>
<p>First, we need to import all the modules we will need:</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
</code></pre>
<p>We use pandas to easily load the CSV homicide data:</p>
<pre><code class="language-python">D = pd.read_csv(&quot;homicide.csv&quot;)
x_data = np.matrix(D.age.values)
y_data = np.matrix(D.num_homicide_deaths.values)
</code></pre>
<p>Note that <code>x_data</code> and <code>y_data</code> are <em>not</em> single numbers, but are actually <a href="https://en.wikipedia.org/wiki/Vector_space">vectors</a>. The vectors are 30 numbers long, since there are 30 data points in the CSV file. So, <code>(x_data[0], y_data[0])</code> would be \((x_1, y_1) = (21, 652)\). When we look at multi variable regression later, we will have to work much more with vectors, matrices and linear algebra, but for now you can think of <code>x_data</code> and <code>y_data</code> just as lists of numbers. Also, we have to use <code>np.matrix(...)</code> to convert the array of numbers <code>D.age.values</code> to an actual numpy vector (likewise for <code>D.num_homicide_deaths.values</code>).</p>
<p>Whenever possible, I would recommend plotting data, since this helps you verify that you loaded the data set correctly and gain visual intuition about the shape of the data. This is also pretty easy using matplotlib:</p>
<pre><code class="language-python">plt.plot(x_data.T, y_data.T, 'x')
plt.xlabel('Age')
plt.ylabel('US Homicide Deaths in 2015')
plt.title('Relationship between age and homicide deaths in the US')
plt.show()
</code></pre>
<p>When we converted the data to vectors using <code>np.matrix()</code>, numpy created vectors with the shape 1 x 30. That is, <code>x_data</code> consists of only 1 row of numbers, and 30 columns. This is actually great for us when working with TensorFlow, but matplotlib wants vectors that have the shape 30 x 1 (30 rows and 1 column). Writing <code>x_data.T</code> calculates the <a href="https://en.wikipedia.org/wiki/Transpose">transpose</a> of <code>x_data</code>, which flips it from a 1 x 30 vector to a 30 x 1 vector. It's fine if you don't understand this now, as we will learn more linear algebra later. Anyways, the plot should look like this:
<img src="/books/tensorflow/book/ch2-linreg/assets/homicide.png" alt="Homicide Plot" /></p>
<p>You need to close the plot for your code to continue executing.</p>
<a class="header" href="ch2-linreg/single-variable.html#defining-the-model" id="defining-the-model"><h3>Defining the model</h3></a>
<p>We have our data prepared and plotted, so now we need to define our model. Recall that the model equation is:
\[
y' = ax + b
\]
Before, we thought of \(x\) and \(y'\) as single numbers. However, we just loaded our data set as vectors (lists of numbers), so it will be much more convenient to define our model using vectors instead of single numbers. If we use the convention that \(x\) and \(y'\) are vectors, then we don't need to change the equation, just our interpretation of it. Multiplying the vector \(x\) by the single number \(a\) just multiplies every number in \(x\) be \(a\), and likewise for adding \(b\). So, the above equation interpreted using vectors is the same thing as:
\[
\begin{bmatrix}
y_{1}', &amp;
y_{2}', &amp;
\dots, &amp;
y_{m}',
\end{bmatrix} = \begin{bmatrix}
ax_{1} + b, &amp;
ax_{2} + b, &amp;
\dots, &amp;
ax_{m} + b
\end{bmatrix}
\]</p>
<p>Fortunately, TensorFlow does the work for us of interpreting the simple equation \(y' = ax + b\) as the more complicated looking vector equation. We just have to tell TensorFlow which things are vectors (\(x\) and \(y'\)), and which are not vectors (\(a\) and \(b\)). First, we define \(x\):</p>
<pre><code class="language-python">x = tf.placeholder(tf.float32, shape=(1, None))
</code></pre>
<p>This says that we create a <strong>placeholder</strong> that stores floating-point numbers, and has a <strong>shape</strong> of 1 x None. The shape of 1 x None tells TensorFlow that \(x\) is a vector with 1 row, and some unspecified number of columns. Although we don't tell TensorFlow the number of columns, this is enough to tell TensorFlow that \(x\) is a vector.</p>
<p>Secondly, note that we create a <code>tf.placeholder</code>: <code>x</code> does not have a numerical value right now. Instead, we will later feed the values of <code>x_data</code> into <code>x</code>. In short, use a <code>tf.placeholder</code> whenever there are values you wish to fill in later (usually data).</p>
<p>Now, we define \(a\) and \(b\):</p>
<pre><code class="language-python">a = tf.get_variable(&quot;a&quot;, shape=(1))
b = tf.get_variable(&quot;b&quot;, shape=(1))
</code></pre>
<p>Unlike <code>x</code>, we create <code>a</code> and <code>b</code> to be a <strong>variable</strong>, instead of a placeholder. The main difference between a variable and a placeholder is that TensorFlow will automatically find the best values of variables by using gradient descent (later). In other words, a placeholder changes values whenever we choose to feed it different numeric values. A variable changes values continually and automatically during gradient descent. Use a variable for something that is <strong>trainable</strong>, that is, something whose optimal value will be found by gradient descent.  Since the goal of linear regression is to find the best values of \(a\) and \(b\), the (only) TensorFlow variables in our model are <code>a</code> and <code>b</code>. The conceptual difference between a TensorFlow placeholder and variable is crucial to using TensorFlow properly.</p>
<p>The parameters <code>(&quot;a&quot;, shape=(1))</code> indicate the name of the variable, and that <code>a</code> is a single number, <em>not</em> a vector. In comparison to <code>x</code>, note that a shape of <code>(1, None)</code> indicates a vector, while a shape of <code>(1)</code> indicates a single number.</p>
<p>With <code>x</code>, <code>a</code> and  <code>b</code> defined, we can define \(y'\):</p>
<pre><code class="language-python">y_predicted = a*x + b
</code></pre>
<p>And that's it to define the model!</p>
<a class="header" href="ch2-linreg/single-variable.html#defining-the-loss-function" id="defining-the-loss-function"><h3>Defining the loss function</h3></a>
<p>We have the model defined, so now we need to define the loss function. Recall that the loss function is how the model is evaluated (smaller loss values are better), and it is also the function that we need to minimize in terms of \(a\) and \(b\).  Since the loss function compares the linear regression output to the correct output, we need to define \(y\), which are the actual output values from the data set. Since \(y\) consists of outside data (and we don't need to train it), we create it as a <code>tf.placeholder</code>:</p>
<pre><code class="language-python">y = tf.placeholder(tf.float32, shape=(1, None))
</code></pre>
<p>Like <code>x</code>, <code>y</code> is also a vector, since after all <code>y</code> must store the correct output for each value stored in <code>x</code>.</p>
<p>Now, we are ready to setup the loss function. Recall that the loss function is:
\[
L(a, b) = \sum_{i=1}^m (y'(x_i, a, b) - y_i)^2
\]</p>
<p>However, \(y'\) and \(y\) are now being interpreted as vectors. We can rewrite the loss function as:
\[
L(a, b) = \mathrm{sum}((y' - y)^2)
\]
Note that since \(y'\) and \(y\) are vectors, \(y' - y\) is also a vector that just contains every number stored in \(y\) subtracted from every corresponding number in \(y'\). Likewise, \((y' - y)^2\) is also a vector, with every number individually squared.  Then, the \(\mathrm{sum}\) function (I just made it up) adds up every number stored in the vector \((y' - y)^2\). This is the same as the original loss function, but is a vector interpretation of it instead. We can code this directly:</p>
<pre><code class="language-python">L = tf.reduce_sum((y_predicted - y)**2)
</code></pre>
<p>The <code>tf.reduce_sum</code> function is an operation which adds up all the numbers stored in a vector. It is called &quot;reduce&quot; since it reduces a large vector down to a single number (the sum). The word &quot;reduce&quot; here has nothing to do with the fact that we will minimize the loss function.</p>
<p>With just these two lines of code we have defined our loss function</p>
<a class="header" href="ch2-linreg/single-variable.html#minimizing-the-loss-function-with-gradient-descent" id="minimizing-the-loss-function-with-gradient-descent"><h3>Minimizing the loss function with gradient descent</h3></a>
<p>With our model and loss function defined, we are now ready to use the gradient descent algorithm to minimize the loss function, and thus find the optimal \(a\) and \(b\). Fortunately, TensorFlow as already implemented the gradient descent algorithm for us, we just need to use it. The algorithm simulates a ball rolling downhill into the minimum of the function, but it does so in discrete time steps. TensorFlow does not handle this aspect, we need to be responsible for performing each time step of gradient descent. So, roughly we want to do this:</p>
<pre><code class="language-python"># Pseudo-code for gradient descent training.
for t in range(10000):
    # Tell TensorFlow to do 1 time step of gradient descent
</code></pre>
<p>We can't do this yet, since we don't yet have a way to tell TensorFlow to perform 1 time step of gradient descent. To do so, we create an optimizer with a learning rate (\(\alpha)\) of \(0.1\):</p>
<pre><code class="language-python">optimizer = tf.train.AdamOptimizer(learning_rate=0.2).minimize(L)
</code></pre>
<p>The <code>tf.train.AdamOptimizer</code> knows how to perform the gradient descent algorithm for us (actually a faster version of gradient descent). Note that this <em>does not yet minimize \(L\)</em>. This code only create an optimizer object which we will use later to minimize \(L\). For an explanation of the <code>learning_rate=0.2</code> parameter (and the <code>10000</code> loop iterations), see the end of this tutorial.</p>
<p>The second problem we have is we don't know how to make TensorFlow run actual computations. Everything so far has been only <em>defining</em> things for TensorFlow, not computing things with concrete numbers. To do so, we also need to create a <strong>session</strong>:</p>
<pre><code class="language-python">session = tf.Session()
</code></pre>
<p>A TensorFlow session is how we always have to perform actual computations with TensorFlow. We actually need to perform a computation right now, before doing gradient descent. Previously, we defined the variables <code>a</code> and <code>b</code>, but they don't have any numeric value right now. They need to have some initial value so gradient descent can work. To solve this, we have TensorFlow initialize <code>a</code> and <code>b</code> with random values:</p>
<pre><code class="language-python">session.run(tf.global_variables_initializer())
</code></pre>
<p>The <code>session.run</code> function is how we always have to run computations with TensorFlow: the parameter is what computation we want to perform.</p>
<p>Finally, we are ready to run the optimization loop pseudo-code that we originally wanted. Using <code>session.run</code> it looks like:</p>
<pre><code class="language-python">for t in range(10000):
    _, current_loss, current_a, current_b = session.run([optimizer, L, a, b], feed_dict={
        x: x_data,
        y: y_data
    })
    print(&quot;t = %g, loss = %g, a = %g, b = %g&quot; % (t, current_loss, current_a, current_b))
</code></pre>
<p>Let's break this down. We use <code>session.run</code>, but we pass it an array of computations that we want to perform. Specifically, we want to perform 4 computations: <code>optimizer</code> (this is 1 time step of gradient descent), <code>L</code> (this returns the current value of the loss function), <code>a</code> (this returns the current value of <code>a</code>), and <code>b</code> (likewise returns the current value of <code>b</code>). Of these computations, only the <code>optimizer</code> does not return a value. So, <code>session.run</code> will return 3 values for us, which we store into variables using the syntax:</p>
<pre><code class="language-python">_, current_loss, current_a, current_b = session.run([optimizer, L, a, b], ...)
</code></pre>
<p>Ok, but what is all of this <code>feed_dict</code> stuff? Recall that <code>x</code> and <code>y</code> are placeholders, and have no actual numerical value on their own. To perform 1 time step of gradient descent, we need to &quot;feed&quot; our actual data (<code>x_data</code> and <code>y_data</code>) into the <code>x</code> and <code>y</code> placeholders. So, we use the <code>feed_dict</code> parameter of <code>session.run</code> to feed <code>x_data</code> into <code>x</code>, and <code>y_data</code> into <code>y</code>.</p>
<p>Finally, the last line of the loop prints out the current values of <code>t</code>, <code>L</code>, <code>a</code> and <code>b</code>. We don't need to print these values out, but it is helpful to observe how the training is progressing.</p>
<p>What we want to see from the print statements is that the gradient descent algorithm <strong>converged</strong>, which means that the algorithm stopped making significant progress because it found the minimum location of the loss function. When the last few print outputs look like:</p>
<pre><code>t = 9992, loss = 39295.8, a = -17.271, b = 997.281
t = 9993, loss = 39295.8, a = -17.271, b = 997.282
t = 9994, loss = 39295.9, a = -17.271, b = 997.282
t = 9995, loss = 39295.9, a = -17.271, b = 997.283
t = 9996, loss = 39295.8, a = -17.2711, b = 997.283
t = 9997, loss = 39295.8, a = -17.2711, b = 997.284
t = 9998, loss = 39295.9, a = -17.2711, b = 997.284
t = 9999, loss = 39295.8, a = -17.2711, b = 997.285
</code></pre>
<p>then we can tell that we have achieved convergence, and therefore found the best values of \(a\) and \(b\).</p>
<a class="header" href="ch2-linreg/single-variable.html#using-the-trained-model-to-make-predictions" id="using-the-trained-model-to-make-predictions"><h3>Using the trained model to make predictions</h3></a>
<p>At this point we have a fully trained model, and know the best values of \(a\) and \(b\). In fact, the equation of the line of best fit is just:
\[
y' = -17.2711x + 997.285
\]</p>
<p>The last remaining thing for this tutorial is to plot the predictions of the model on top of a plot of the data. First, we need to create a bunch of input ages that we will predict the homicide rates for. We could use <code>x_data</code> as the input ages, but it is more interesting to create a new vector of input ages, since then we can predict homicide rates even for ages that were not in the data set. We can use the numpy function <code>linspace</code> to create a bunch of evenly spaced values:</p>
<pre><code class="language-python"># x_test_data has values similar to [20.0, 20.1, 20.2, ..., 54.9, 55.0]
x_test_data = np.matrix(np.linspace(20, 55))
</code></pre>
<p>Then, we can feed <code>x_test_data</code> into the <code>x</code> placeholder, and save the outputs of the prediction in <code>y_test_data</code>:</p>
<pre><code class="language-python">y_test_data = session.run(y_predicted, feed_dict={
    x: x_test_data
})
</code></pre>
<p>Finally, we can plot the original data and the line together:</p>
<pre><code class="language-python">plt.plot(x_data.T, y_data.T, 'x')
plt.plot(x_test_data.T, y_test_data.T)
plt.xlabel('Age')
plt.ylabel('US Homicide Deaths in 2015')
plt.title('Age and homicide death linear regression')
plt.show()
</code></pre>
<p>This yields a plot like:</p>
<p><img src="/books/tensorflow/book/ch2-linreg/assets/homicide_fit.png" alt="Homicide Linear Regression Plot" /></p>
<a class="header" href="ch2-linreg/single-variable.html#concluding-remarks" id="concluding-remarks"><h1>Concluding Remarks</h1></a>
<p>Through following this post you have learned two main concepts. First, you learned the <em>general form of supervised machine learning workflows</em>:</p>
<ol>
<li>Get your data set</li>
<li>Define your model (the mechanism for how outputs will be predicted from inputs)</li>
<li>Define your loss function</li>
<li>Minimize your loss function (usually with a variant of gradient descent)</li>
<li>Once your loss function is minimized, use your trained model to do cool stuff</li>
</ol>
<p>Second, you learned how to implement linear regression (following the above workflow) using TensorFlow. Let's briefly discuss the above 5 steps, and where to go to improve on them.</p>
<a class="header" href="ch2-linreg/single-variable.html#1-the-data-set" id="1-the-data-set"><h2>1. The Data Set</h2></a>
<p>This one is pretty simple: we need data sets that contain both input and output data. However, we need a data set that is large enough to properly train our model. With linear regression this is fairly easy: this data set only had 33 data points, and the results were pretty good. With larger and more complex models that we will look at later, this becomes much more of a challenge.</p>
<a class="header" href="ch2-linreg/single-variable.html#2-defining-the-model" id="2-defining-the-model"><h2>2. Defining the model</h2></a>
<p>For single variable linear regression we used the model \(y' = ax + b\). Geometrically, this means that the model can only guess lines. Since the homicide data is roughly in the shape of a line, it worked well for this problem. But there are very few problems that are so simple, so soon we will look at more complex models. One other limitation of the current model is it only accepts one input variable. But if our data set had both age and ethnicity, for example, perhaps we could more accurately predict homicide death rate. Also very soon we will discuss a more complex model that handles multiple input variables.</p>
<a class="header" href="ch2-linreg/single-variable.html#3-defining-the-loss-function" id="3-defining-the-loss-function"><h2>3. Defining the loss function</h2></a>
<p>For single variable regression, the loss function we used, \(L = \sum (y' - y)^2\), is the standard. However, there are a few considerations: first, this loss functions is suitable for this simple model, but with more advanced models this loss function isn't good enough. We will see why soon. Second, the optimization algorithm converged pretty slowly, needing about \(10000\) iterations. The cause is that the surface of the loss function is almost flat in a certain direction (you can see this in the 3D plot of it). Though this isn't a problem with the formula for the loss function, the problem surfaces in the loss function. We will also see how to address this problem soon, and converge muster faster.</p>
<a class="header" href="ch2-linreg/single-variable.html#4-minimizing-the-loss-functions" id="4-minimizing-the-loss-functions"><h2>4. Minimizing the loss functions</h2></a>
<p>Recall that we created and used the optimizer like so:</p>
<pre><code class="language-python">optimizer = tf.train.AdamOptimizer(learning_rate=0.2).minimize(L)
for t in range(10000):
    # Run one step of optimizer
</code></pre>
<p>You might be wondering what the magic numbers of <code>learning_rate=0.2</code> (\( \alpha \)) and <code>10000</code> are.  Let's start with the learning rate. In each iteration, gradient descent (and variants of it) take one small step that is determined by the derivative of the loss function. The learning rate is just the relative size of the step. So to take larger steps, we can use a larger learning rate. A larger learning rate can help us to converge more quickly, since we cover more distance in each iteration. But a learning rate too large can cause gradient descent to diverge that is, it won't reliably find the minimum.</p>
<p>So, once you have chosen a learning rate, then you need to run the optimizer for enough iterations so it actually converges to the minimum. The easiest way to make sure it runs long enough is just to monitor the value of the loss function, as we did in this tutorial.</p>
<p>Lastly, we didn't use normal gradient descent for optimization in this tutorial. Instead we used <code>tf.train.AdamOptimizer</code>. With small scale problems like this, there isn't much of a qualitative difference to intuit. In general the <a href="https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218">Adam optimizer</a> is faster, smarter, and more reliable than vanilla gradient descent, but this comes into play a lot more with harder problems.</p>
<a class="header" href="ch2-linreg/single-variable.html#5-use-the-trained-model" id="5-use-the-trained-model"><h2>5. Use the trained model</h2></a>
<p>Technically, using the trained model is the easiest part of machine learning: with the best parameters \(a\) and \(b\), you can simply plug new age values into \(x\) to predict new homicide rates. However, trusting that these predictions are correct is another matter entirely. Later in this book we can look at various statistical techniques that can help determine how much we can trust a trained model, but for now consider some oddities with our trained homicide rate model.</p>
<p>One rather weird thing is that it accepts negative ages: according to the model, 1083 people who are -5 years old die from homicide every year in the US. Now, clearly this makes no sense since people don't have negative ages. So perhaps we should only let the model be valid for people with positives ages. Ok, so then 980 people who are 1 year old die from homicide every year. While this isn't impossible, it does seem pretty high compared to the known data of 652 for 21 year olds. It might seem possible (likely even) that fewer homicides occur for 1 year olds than 21 year olds: but we don't have the data for that, and even if we did, our model could not predict it correctly since it only models straight lines. Without more data, we have no basis to conclude that the number of \(1\) year old homicides is even close to 980.</p>
<p>While this might seem like a simple observation in this case, this problem manifests itself continually in machine learning, causing a variety of ethical problems. For example, in 2016 Microsoft released a chatbot on Twitter and <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">it quickly learned to say fairly horrible and racist things</a>. More seriously, machine learning is now being used to predict and guide police in cracking down on crime. While the concept might be well-intentioned, the results are despicable, according to <a href="http://theconversation.com/why-big-data-analysis-of-police-activity-is-inherently-biased-72640">an article by The Conversation</a>:</p>
<blockquote>
<p>Our recent study, by Human Rights Data Analysis Group’s Kristian Lum and William Isaac, found that predictive policing vendor PredPol’s purportedly race-neutral algorithm targeted black neighborhoods at roughly twice the rate of white neighborhoods when trained on historical drug crime data from Oakland, California.
[...]
But estimates – created from public health surveys and population models – suggest illicit drug use in Oakland is roughly equal across racial and income groups. If the algorithm were truly race-neutral, it would spread drug-fighting police attention evenly across the city.</p>
</blockquote>
<p>With examples like these, we quickly move from a technical discussion about machine learning to a discussion about ethics. While the study of machine learning is traditionally heavily theoretical, I strongly believe that to effectively and <em>fairly</em> apply machine learning in society, we must spend significant effort evaluating the ethics of machine learning models.</p>
<p>This is an open question, and one that I certainly don't have an answer to right now. For the short term we can focus on the problem of not trusting a simple linear regression model to properly predict data outside of what it has been trained on, while in the long term keeping in mind that &quot;with great power comes great responsibility.&quot;</p>
<a class="header" href="ch2-linreg/single-variable.html#exercises" id="exercises"><h1>Exercises</h1></a>
<p>Feel free to complete as many of these as you wish, to get more practice with single variable linear regression. Note that for different problems you might have to adjust the learning rate and / or the number of training iterations.</p>
<ol>
<li>Learn how to use numpy to generate an artificial data set that is appropriate for single variable linear regression, and then train a model on it. As a hint, for any \(x\) value you could create an artificial \(y\) value like so: \(y = ax + b + \epsilon \), where \(\epsilon\) is a random number that isn't too big, and \(a\) and \(b\) are fixed constants of your choice. If done correctly, your trained model should learn by itself the numbers you chose for \(a\) and \(b\).</li>
<li>Run single variable linear regression on a data set of your choice. You can look at <a href="http://donaldpinckney.com/ml.html#regression">my list of regression data sets</a> for ideas, you can search <a href="https://www.kaggle.com/datasets">Kaggle</a>, or you can search online, such as I did for the homicide data set. Many data sets might have multiple input variables, and right now you only know how to do single variable linear regression. We will deal with multiple variables soon, but for now you can always use only 1 of the input variables and ignore the rest.</li>
</ol>
<a class="header" href="ch2-linreg/single-variable.html#complete-code" id="complete-code"><h1>Complete Code</h1></a>
<p>The <a href="https://github.com/donald-pinckney/donald-pinckney.github.io/blob/src/books/tensorflow/src/ch2-linreg/code/single_var_reg.py">complete example code is available on GitHub</a>, as well as directly here:</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt

# Load the data, and convert to 1x30 vectors
D = pd.read_csv(&quot;homicide.csv&quot;)
x_data = np.matrix(D.age.values)
y_data = np.matrix(D.num_homicide_deaths.values)

# Plot the data
plt.plot(x_data.T, y_data.T, 'x')
plt.xlabel('Age')
plt.ylabel('US Homicide Deaths in 2015')
plt.title('Relationship between age and homicide deaths in the US')
plt.show()



### Model definition ###

# Define x (input data) placeholder
x = tf.placeholder(tf.float32, shape=(1, None))

# Define the trainable variables
a = tf.get_variable(&quot;a&quot;, shape=(1))
b = tf.get_variable(&quot;b&quot;, shape=(1))

# Define the prediction model
y_predicted = a*x + b


### Loss function definition ###

# Define y (correct data) placeholder
y = tf.placeholder(tf.float32, shape=(1, None))

# Define the loss function
L = tf.reduce_sum((y_predicted - y)**2)


### Training the model ###

# Define optimizer object
optimizer = tf.train.AdamOptimizer(learning_rate=0.2).minimize(L)

# Create a session and initialize variables
session = tf.Session()
session.run(tf.global_variables_initializer())

# Main optimization loop
for t in range(10000):
    _, current_loss, current_a, current_b = session.run([optimizer, L, a, b], feed_dict={
        x: x_data,
        y: y_data
    })
    print(&quot;t = %g, loss = %g, a = %g, b = %g&quot; % (t, current_loss, current_a, current_b))


### Using the trained model to make predictions ###

# x_test_data has values similar to [20.0, 20.1, 20.2, ..., 79.9, 80.0]
x_test_data = np.matrix(np.linspace(20, 55))

# Predict the homicide rate for each age in x_test_data
y_test_data = session.run(y_predicted, feed_dict={
    x: x_test_data
})

# Plot the original data and the prediction line
plt.plot(x_data.T, y_data.T, 'x')
plt.plot(x_test_data.T, y_test_data.T)
plt.xlabel('Age')
plt.ylabel('US Homicide Deaths in 2015')
plt.title('Age and homicide death linear regression')
plt.show()
</code></pre>
<a class="header" href="ch2-linreg/single-variable.html#references" id="references"><h1>References</h1></a>
<div class="footnote-definition" id="fn1"><sup class="footnote-definition-label">1</sup>
<p>Centers for Disease Control and Prevention, National Center for Health Statistics. Underlying Cause of Death 1999-2015 on CDC WONDER Online Database, released December, 2016. Data are from the Multiple Cause of Death Files, 1999-2015, as compiled from data provided by the 57 vital statistics jurisdictions through the Vital Statistics Cooperative Program. Accessed at <a href="http://wonder.cdc.gov/ucd-icd10.html">http://wonder.cdc.gov/ucd-icd10.html</a> on Nov 22, 2017 2:18:46 PM.</p>
</div>

                </div>

                <!-- Mobile navigation buttons -->
                
                    <a rel="prev" href="ch2-linreg/intro.html" class="mobile-nav-chapters previous">
                        <i class="fa fa-angle-left"></i>
                    </a>
                

                
                    <a rel="next" href="http:/index.html" class="mobile-nav-chapters next">
                        <i class="fa fa-angle-right"></i>
                    </a>
                

            </div>

            
                <a href="ch2-linreg/intro.html" class="nav-chapters previous" title="You can navigate through the chapters using the arrow keys">
                    <i class="fa fa-angle-left"></i>
                </a>
            

            
                <a href="http:/index.html" class="nav-chapters next" title="You can navigate through the chapters using the arrow keys">
                    <i class="fa fa-angle-right"></i>
                </a>
            

        </div>


        <!-- Local fallback for Font Awesome -->
        <script>
            if ($(".fa").css("font-family") !== "FontAwesome") {
                $('<link rel="stylesheet" type="text/css" href="_FontAwesome/css/font-awesome.css">').prependTo('head');
            }
        </script>

        <!-- Livereload script (if served using the cli tool) -->
        

        

        

        

        <script src="highlight.js"></script>
        <script src="book.js"></script>
    </body>
</html>
